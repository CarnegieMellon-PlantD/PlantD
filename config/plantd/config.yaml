name: plantd
version: 1.0.0
pvc.requests.storage: 10Gi
dataGenerator:
  path: /plantd
  image: docker.io/datawindtunnel/datageneratorjob:v1demo6
  restartPolicy: Never
  backoffLimit: 1
  requests.cpu: 1
  limits.cpu: 1
plantdCore:
  kubeProxy:
    image: docker.io/datawindtunnel/kube-frontend-proxy:2
    label: 
      app: kube-proxy
    selector: 
      key: app
      value: kube-proxy
    containerName: kube-proxy
    replicas: 1,
    deploymentName: kube-proxy-deployment
    serviceName: kube-proxy-service
    port: 5000
    targetPort: 5000
  studio:
    image: docker.io/datawindtunnel/frontend:latest
    label: 
      app: frontend
    selector: 
      key: app
      value: frontend
    containerName: frontend
    replicas: 1,
    deploymentName: frontend-deployment
    serviceName: frontend-service
    portName: http
    port: 80
    targetPort: 80
  prometheus:
    name: prometheus
    serviceAccountname: prometheus
    resourceMemory: 1G
    scrapeInterval: 15s
    clusteRoleName: plantd-prometheus-role
    clusterRoleBindingName: plantd-prometheus-role-binding
    selector:
      key: component
      value: plantd-metrics-endpoint
    service:
      port: 9090
      nodePort: 30900
    selector:
      prometheus: prometheus
k6:
  arguments: --out experimental-prometheus-rw
  remoteWriteURL:
    name: K6_PROMETHEUS_RW_SERVER_URL
    value: http://prometheus:9090/api/v1/write
  script.http.withData: |
    import http from 'k6/http';
    import { check } from 'k6';

    let loadpattern = JSON.parse(open('loadpattern.json'));
    let pipeline = JSON.parse(open('pipeline.json'));
    const url = pipeline.http.url;
    const method = pipeline.http.method;
    const data = JSON.stringify(pipeline.http.body.data || "");
    const headers = pipeline.http.headers || {};
    export let options = {
      scenarios: {
        ramping_arrival_rate: {
          executor: 'ramping-arrival-rate',
          startRate: loadpattern.startRate,
          timeUnit: loadpattern.timeUnit,
          preAllocatedVUs: loadpattern.preAllocatedVUs,
          maxVUs: loadpattern.maxVUs,
          stages: loadpattern.stages,
        },
      },
      discardResponseBodies: true,
      noVUConnectionReuse: true,
    };

    export default function () {
      let res = http.request(method, url, data, {
        headers: headers,
      });
      check(res, {
        'status was 200': (r) => r.status === 200,
      });
    }
  script.http.withDataSet: |
    import http from 'k6/http';
    import { check } from 'k6';
    import { randomIntBetween } from "https://jslib.k6.io/k6-utils/1.4.0/index.js";

    let loadpattern = JSON.parse(open('loadpattern.json'));
    let pipeline = JSON.parse(open('pipeline.json'));
    let dataset = JSON.parse(open('dataset.json'));
    const url = pipeline.http.url;
    const method = pipeline.http.method;
    const headers = pipeline.http.headers || {};
    const numFiles = dataset.spec.numFiles;
    const numSchemas = dataset.spec.schemas.length;
    const compressedFileFormat = dataset.spec.compressedFileFormat || "";
    const fileFormat = dataset.spec.fileFormat;
    const compressPerSchema = dataset.spec.compressPerSchema || false;
    const datasetName = dataset.metadata.name;
    const fileExtention = {
      zip: 'zip',
      binary: 'bin'
    };
    let maxIndex;

    const ext = fileExtention[fileFormat];

    function filePerSchemaArray() {
      const n = numSchemas * numFiles;
      const arr = new Array(n);
      for (let i = 0; i < numSchemas; i++) {
        let k = i * numFiles;
        let schemaName = dataset.spec.schemas[i].name;
        for (let j = 0; j < numFiles; j++) {
          const fname = `${schemaName}/${datasetName}_${schemaName}_${j}.${ext}`;
          arr[k + j] = {
            name: fname,
            content: open(fname, 'b')
          };
        }
      }
      return arr;
    }

    function filepathPerCompressedArray() {
      const arr = new Array(numFiles);
      for (let i = 0; i < numFiles; i++) {
        const fname = `${datasetName}_${i}.${compressedFileFormat}`;
        arr[i] = {
          name: fname,
          content: open(fname, 'b')
        };
      }
      return arr;
    }

    function filepathPerCompressedPerSechmaArray() {
      const n = numSchemas * numFiles;
      const arr = new Array(n);
      for (let i = 0; i < numSchemas; i++) {
        let k = i * numFiles;
        let schemaName = dataset.spec.schemas[i].name;
        for (let j = 0; j < numFiles; j++) {
          const fname = `${schemaName}/${datasetName}_${schemaName}_${j}.${compressedFileFormat}`;
          arr[k + j] = {
            name: fname,
            content: open(fname, 'b')
          };
        }
      }
      return arr;
    }

    let dataCache;

    if (compressedFileFormat === "") {
      maxIndex = numSchemas * numFiles - 1;
      dataCache = filePerSchemaArray()
    } else if (compressPerSchema === true) {
      maxIndex = numSchemas * numFiles - 1;
      dataCache = filepathPerCompressedPerSechmaArray()
    } else {
      maxIndex = numFiles - 1;
      dataCache = filepathPerCompressedArray()
    }

    export let options = {
      scenarios: {
        ramping_arrival_rate: {
          executor: 'ramping-arrival-rate',
          startRate: loadpattern.startRate,
          timeUnit: loadpattern.timeUnit,
          preAllocatedVUs: loadpattern.preAllocatedVUs,
          maxVUs: loadpattern.maxVUs,
          stages: loadpattern.stages,
        },
      },
      discardResponseBodies: true,
      noVUConnectionReuse: true,
    };

    export default function () {
      const i = randomIntBetween(0, maxIndex)
      console.log(dataCache[i]['content'])
      let payload = {
        file: http.file(dataCache[i]['content'], dataCache[i]['name'], 'multipart/form-data'),
      };
      let res = http.request(method, url, payload, {
        headers: headers,
      });
      check(res, {
        'status was 200': (r) => r.status === 200,
      });
    }
  config:
    script: script.js
    loadPattern: loadpattern.json
    pipeline: pipeline.json
    dataset: dataset.json
monitor:
  metricsService:
    labels:
      key: plantd-pipeline
  serviceMonitor:
    labels:
      component: plantd-metrics-endpoint
  pipelineEndpoint:
    labels:
      plantd-component: pipeline-endpoint
  metrics:
    port.name: metrics
  jobLabel: experiment
database:
  prometheus:
    url: http://prometheus:9090
    scrapeInterval: 15s
