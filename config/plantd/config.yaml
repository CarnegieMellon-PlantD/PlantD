name: PlantD
version: 1.0.0
dataGenerator:
  defaultImage: ghcr.io/carnegiemellon-plantd/datagenerator:latest
  defaultParallelism: 1
  backoffLimit: 0
  defaultStorageSize: 2Gi
  # We use the default path where K6 looks for files, do not change it
  path: /test
costService:
  image: ghcr.io/carnegiemellon-plantd/costexporter:latest
  opencost:
    url: http://opencost.plantd-operator-system.svc.cluster.local:9003
digitalTwin:
  image: ghcr.io/carnegiemellon-plantd/digitaltwin:latest
plantDCore:
  serviceAccountName: plantd-operator-controller-manager
  kubeProxy:
    labels:
      app: plantd-proxy
    containerName: plantd-proxy
    image: ghcr.io/carnegiemellon-plantd/plantd-proxy:latest
    deploymentName: plantd-proxy-deployment
    replicas: 1
    serviceName: plantd-proxy-service
    port: 5000
    targetPort: 5000
  studio:
    labels:
      app: plantd-studio
    containerName: plantd-studio
    image: ghcr.io/carnegiemellon-plantd/plantd-studio:latest
    deploymentName: plantd-studio-deployment
    replicas: 1
    serviceName: plantd-studio-service
    port: 80
    targetPort: 8080
  prometheus:
    labels:
      prometheus: prometheus
    serviceMonitorSelector:
      component: plantd-metrics-endpoint
    serviceAccountName: prometheus
    clusterRoleName: plantd-prometheus-role
    clusterRoleBindingName: plantd-prometheus-role-binding
    name: prometheus
    scrapeInterval: 15s
    replicas: 1
    memoryLimit: 1Gi
    serviceName: prometheus-service
    port: 9090
    targetPort: 9090
    nodePort: 30900
    securityContext:
      runAsUser: 1000
      fsGroup: 2000
      runAsGroup: 2000
      runAsNonRoot: true
    thanos:
      thanosBaseImage: quay.io/thanos/thanos
      thanosVersion: v0.34.0
      thanosConfig:
        name: thanos-objstore-config
        key: thanos.yaml
    thanosQuerier:
      deploymentName: thanos-querier
      labels:
        app: thanos-querier
      image: quay.io/thanos/thanos:v0.34.0
      replicas: 1
      httpPort: 9090
      grpcPort: 10901
      url: thanos-sidecar-grpc.plantd-operator-system.svc.cluster.local:10901
    thanosStore:
      name: thanos-store
      serviceName: thanos-store-grpc
      labels:
        app: thanos-store
      image: quay.io/thanos/thanos:v0.34.0
      replicas: 1
      volumeSize: 1Gi
      dataDir: /thanos-data
      httpPort: 10902
      grpcPort: 10901
      url: thanos-store-grpc.plantd-operator-system.svc.cluster.local:10901
      securityContext:
        fsGroup: 65534
        runAsGroup: 65532
        runAsUser: 65534
    thanosSidecarService:
      portName: grpc
      port: 10901

  redis:
    labels:
      app: redis
    containerName: redis
    image: redis/redis-stack-server:latest
    deploymentName: redis-deployment
    replicas: 1
    serviceName: redis-service
    port: 6379
    targetPort: 6379

opencost:
  serviceAccount: opencost
  clusterRole: opencost
  deployment:
    image: quay.io/kubecost1/kubecost-cost-model:latest
    ui-image: quay.io/kubecost1/opencost-ui:1.105.2
    replicas: 1
    maxSurge: 1
    maxUnavailable: 1
    runAsUser: 1001
    labels:
      plantd-opencost: opencost
  service:
    port: 9003
    ui-port: 9090
    labels:
      plantd-opencost: opencost
  opencostServiceMonitor:
    name: opencost-service-monitor
    labels:
      component: plantd-metrics-endpoint
    selector:
      plantd-opencost: opencost
  cadvisorServiceMonitor:
    name: cadvisor-service-monitor
    labels:
      component: plantd-metrics-endpoint
    selector:
      k8s-app: kubelet
    namespaceSelector: kube-system

k6:
  arguments: --out experimental-prometheus-rw
  remoteWriteURL:
    name: K6_PROMETHEUS_RW_SERVER_URL
    # Ensure Prometheus service name, namespace, and port are correct
    value: http://prometheus-service.plantd-operator-system.svc.cluster.local:9090/api/v1/write
  # Sync with `apps/loadgen/http_with_data.js`
  script.http.withData: |
    import http from 'k6/http';
    import { check } from 'k6';

    let loadpattern = JSON.parse(open('loadpattern.json'));
    let pipeline = JSON.parse(open('pipeline.json'));
    const url = pipeline.http.url;
    const method = pipeline.http.method;
    const data = JSON.stringify(pipeline.http.body.data || "");
    const headers = pipeline.http.headers || {};
    export let options = {
      scenarios: {
        ramping_arrival_rate: {
          executor: 'ramping-arrival-rate',
          startRate: loadpattern.startRate,
          timeUnit: loadpattern.timeUnit,
          preAllocatedVUs: loadpattern.preAllocatedVUs,
          maxVUs: loadpattern.maxVUs,
          stages: loadpattern.stages,
        },
      },
      discardResponseBodies: true,
      noVUConnectionReuse: true,
    };

    export default function () {
      let res = http.request(method, url, data, {
        headers: headers,
      });
      check(res, {
        'status was 200': (r) => r.status === 200,
      });
    }
  # Sync with `apps/loadgen/http_with_dataset.js`
  script.http.withDataSet: |
    import http from 'k6/http';
    import { check } from 'k6';
    import { randomIntBetween } from "https://jslib.k6.io/k6-utils/1.4.0/index.js";

    let loadpattern = JSON.parse(open('loadpattern.json'));
    let pipeline = JSON.parse(open('pipeline.json'));
    let dataset = JSON.parse(open('dataset.json'));
    const url = pipeline.http.url;
    const method = pipeline.http.method;
    const headers = pipeline.http.headers || {};
    const numFiles = dataset.spec.numFiles;
    const numSchemas = dataset.spec.schemas.length;
    const compressedFileFormat = dataset.spec.compressedFileFormat || "";
    const fileFormat = dataset.spec.fileFormat;
    const compressPerSchema = dataset.spec.compressPerSchema || false;
    const datasetName = dataset.metadata.name;
    const fileExtention = {
      zip: 'zip',
      binary: 'bin'
    };
    let maxIndex;

    const ext = fileExtention[fileFormat];

    function filePerSchemaArray() {
      const n = numSchemas * numFiles;
      const arr = new Array(n);
      for (let i = 0; i < numSchemas; i++) {
        let k = i * numFiles;
        let schemaName = dataset.spec.schemas[i].name;
        for (let j = 0; j < numFiles; j++) {
          const fname = `${schemaName}/${datasetName}_${schemaName}_${j}.${ext}`;
          arr[k + j] = {
            name: fname,
            content: open(fname, 'b')
          };
        }
      }
      return arr;
    }

    function filepathPerCompressedArray() {
      const arr = new Array(numFiles);
      for (let i = 0; i < numFiles; i++) {
        const fname = `${datasetName}_${i}.${compressedFileFormat}`;
        arr[i] = {
          name: fname,
          content: open(fname, 'b')
        };
      }
      return arr;
    }

    function filepathPerCompressedPerSechmaArray() {
      const n = numSchemas * numFiles;
      const arr = new Array(n);
      for (let i = 0; i < numSchemas; i++) {
        let k = i * numFiles;
        let schemaName = dataset.spec.schemas[i].name;
        for (let j = 0; j < numFiles; j++) {
          const fname = `${schemaName}/${datasetName}_${schemaName}_${j}.${compressedFileFormat}`;
          arr[k + j] = {
            name: fname,
            content: open(fname, 'b')
          };
        }
      }
      return arr;
    }

    let dataCache;

    if (compressedFileFormat === "") {
      maxIndex = numSchemas * numFiles - 1;
      dataCache = filePerSchemaArray()
    } else if (compressPerSchema === true) {
      maxIndex = numSchemas * numFiles - 1;
      dataCache = filepathPerCompressedPerSechmaArray()
    } else {
      maxIndex = numFiles - 1;
      dataCache = filepathPerCompressedArray()
    }

    export let options = {
      scenarios: {
        ramping_arrival_rate: {
          executor: 'ramping-arrival-rate',
          startRate: loadpattern.startRate,
          timeUnit: loadpattern.timeUnit,
          preAllocatedVUs: loadpattern.preAllocatedVUs,
          maxVUs: loadpattern.maxVUs,
          stages: loadpattern.stages,
        },
      },
      discardResponseBodies: true,
      noVUConnectionReuse: true,
    };

    export default function () {
      const i = randomIntBetween(0, maxIndex)
      console.log(dataCache[i]['content'])
      let payload = {
        file: http.file(dataCache[i]['content'], dataCache[i]['name'], 'multipart/form-data'),
      };
      let res = http.request(method, url, payload, {
        headers: headers,
      });
      check(res, {
        'status was 200': (r) => r.status === 200,
      });
    }
  config:
    script: script.js
    loadPattern: loadpattern.json
    pipeline: pipeline.json
    dataset: dataset.json
monitor:
  metricsService:
    labels:
      key: plantd-monitoring
  serviceMonitor:
    labels:
      component: plantd-metrics-endpoint
  pipelineEndpoint:
    labels:
      plantd-component: pipeline-endpoint
  metrics:
    port.name: metrics
  jobLabel: experiment
database:
  prometheus:
    # Ensure Prometheus service name, namespace, and port are correct
    url: http://prometheus-service.plantd-operator-system.svc.cluster.local:9090
    thanosUrl: http://thanos-querier.plantd-operator-system.svc.cluster.local:9090
  redis:
    # Ensure Redis service name, namespace, and port are correct
    host: redis-service.plantd-operator-system.svc.cluster.local
    port: 6379